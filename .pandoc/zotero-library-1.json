[
  {"id":"AddonItem","citation-key":"AddonItem","title":"Addon item","type":"software"},
  {"id":"bertschIncontextLearningLongcontext2024","abstract":"As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data. We use this ICL setting as a testbed to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts we see do not arise from cumulative gain from encoding many examples together. We conclude that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning.","accessed":{"date-parts":[["2024",7,20]]},"author":[{"family":"Bertsch","given":"Amanda"},{"family":"Ivgi","given":"Maor"},{"family":"Alon","given":"Uri"},{"family":"Berant","given":"Jonathan"},{"family":"Gormley","given":"Matthew R."},{"family":"Neubig","given":"Graham"}],"citation-key":"bertschIncontextLearningLongcontext2024","issued":{"date-parts":[["2024",4,30]]},"language":"en-US","number":"arXiv:2405.00200","publisher":"arXiv","source":"arXiv.org","title":"In-context learning with long-context models: An in-depth exploration","title-short":"In-Context Learning with Long-Context Models","type":"article","URL":"http://arxiv.org/abs/2405.00200"},
  {"id":"caiLargeLanguageModels2023","abstract":"Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the tasks. Furthermore, the division of labor among LLMs for tool-making and tool-using phases introduces the opportunity to achieve cost effectiveness without degrading the quality of generated tools and problem solutions. For example, recognizing that tool-making demands more sophisticated capabilities than tool-using, we can apply a powerful yet resource-intensive model as the tool maker, and a lightweight while cost-effective model as the tool user. We validate the effectiveness of our approach across a variety of complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM can achieve performance that is on par with using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced.","accessed":{"date-parts":[["2023",12,6]]},"author":[{"family":"Cai","given":"Tianle"},{"family":"Wang","given":"Xuezhi"},{"family":"Ma","given":"Tengyu"},{"family":"Chen","given":"Xinyun"},{"family":"Zhou","given":"Denny"}],"citation-key":"caiLargeLanguageModels2023","issued":{"date-parts":[["2023",5,26]]},"language":"en","number":"arXiv:2305.17126","publisher":"arXiv","source":"arXiv.org","title":"Large Language Models as Tool Makers","type":"article","URL":"http://arxiv.org/abs/2305.17126"},
  {"id":"chanDataDistributionalProperties2022","abstract":"Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.","accessed":{"date-parts":[["2024",3,14]]},"author":[{"family":"Chan","given":"Stephanie C. Y."},{"family":"Santoro","given":"Adam"},{"family":"Lampinen","given":"Andrew K."},{"family":"Wang","given":"Jane X."},{"family":"Singh","given":"Aaditya"},{"family":"Richemond","given":"Pierre H."},{"family":"McClelland","given":"Jay"},{"family":"Hill","given":"Felix"}],"citation-key":"chanDataDistributionalProperties2022","issued":{"date-parts":[["2022",11,17]]},"language":"en-US","number":"arXiv:2205.05055","publisher":"arXiv","source":"arXiv.org","title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers","type":"article","URL":"http://arxiv.org/abs/2205.05055"},
  {"id":"chenHowTransformersUtilize2024","abstract":"Despite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood. Recent studies have suggested that transformers can implement gradient descent as an in-context learner for linear regression problems and have developed various theoretical analyses accordingly. However, these works mostly focus on the expressive power of transformers by designing specific parameter constructions, lacking a comprehensive understanding of their inherent working mechanisms post-training. In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning. We experimentally discover that the utilization of multi-heads exhibits different patterns across layers: multiple heads are utilized and essential in the first layer, while usually one single head is dominantly utilized for subsequent layers. We provide a theoretical rationale for this observation: the first layer undertakes data preprocessing on the context examples, and the following layers execute simple optimization steps based on the preprocessed context. Moreover, we prove that such a preprocess-then-optimize algorithm can outperform naive gradient descent and ridge regression algorithms, which is also supported by our further experiments. Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.","accessed":{"date-parts":[["2024",9,10]]},"author":[{"family":"Chen","given":"Xingwu"},{"family":"Zhao","given":"Lei"},{"family":"Zou","given":"Difan"}],"citation-key":"chenHowTransformersUtilize2024","issued":{"date-parts":[["2024",8,8]]},"language":"en","number":"arXiv:2408.04532","publisher":"arXiv","source":"arXiv.org","title":"How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression","title-short":"How Transformers Utilize Multi-Head Attention in In-Context Learning?","type":"article","URL":"http://arxiv.org/abs/2408.04532"},
  {"id":"friedmanLearningTransformerPrograms2023","abstract":"Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck-languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the ``circuits'' used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Friedman","given":"Dan"},{"family":"Wettig","given":"Alexander"},{"family":"Chen","given":"Danqi"}],"citation-key":"friedmanLearningTransformerPrograms2023","issued":{"date-parts":[["2023",6,1]]},"language":"en","number":"arXiv:2306.01128","publisher":"arXiv","source":"arXiv.org","title":"Learning Transformer Programs","type":"article","URL":"http://arxiv.org/abs/2306.01128"},
  {"id":"furuyaTransformersAreUniversal2024","abstract":"Transformers are deep architectures that deﬁne “in-context mappings” which enable predicting new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of patches for vision transformers). This work studies in particular the ability of these architectures to handle an arbitrarily large number of context tokens. To mathematically and uniformly address the expressivity of these architectures, we consider the case that the mappings are conditioned on a context represented by a probability distribution of tokens (discrete for a ﬁnite number of tokens). The related notion of smoothness corresponds to continuity in terms of the Wasserstein distance between these contexts. We demonstrate that deep transformers are universal and can approximate continuous in-context mappings to arbitrary precision, uniformly over compact token domains. A key aspect of our results, compared to existing ﬁndings, is that for a ﬁxed precision, a single transformer can operate on an arbitrary (even inﬁnite) number of tokens. Additionally, it operates with a ﬁxed embedding dimension of tokens (this dimension does not increase with precision) and a ﬁxed number of heads (proportional to the dimension). The use of MLP layers between multi-head attention layers is also explicitly controlled.","accessed":{"date-parts":[["2024",9,18]]},"author":[{"family":"Furuya","given":"Takashi"},{"family":"Hoop","given":"Maarten V.","non-dropping-particle":"de"},{"family":"Peyré","given":"Gabriel"}],"citation-key":"furuyaTransformersAreUniversal2024","issued":{"date-parts":[["2024",8,2]]},"language":"en-US","number":"arXiv:2408.01367","publisher":"arXiv","source":"arXiv.org","title":"Transformers are universal in-context learners","type":"article","URL":"http://arxiv.org/abs/2408.01367"},
  {"id":"haoToolkenGPTAugmentingFrozen2023","abstract":"Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\\underline{tool}$ as a to$\\underline{ken}$ ($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Hao","given":"Shibo"},{"family":"Liu","given":"Tianyang"},{"family":"Wang","given":"Zhen"},{"family":"Hu","given":"Zhiting"}],"citation-key":"haoToolkenGPTAugmentingFrozen2023","issued":{"date-parts":[["2023",6,22]]},"language":"en","number":"arXiv:2305.11554","publisher":"arXiv","source":"arXiv.org","title":"ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings","title-short":"ToolkenGPT","type":"article","URL":"http://arxiv.org/abs/2305.11554"},
  {"id":"levyDiverseDemonstrationsImprove2023","abstract":"In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with finetuning.","accessed":{"date-parts":[["2024",8,7]]},"author":[{"family":"Levy","given":"Itay"},{"family":"Bogin","given":"Ben"},{"family":"Berant","given":"Jonathan"}],"citation-key":"levyDiverseDemonstrationsImprove2023","issued":{"date-parts":[["2023",6,24]]},"language":"en-US","number":"arXiv:2212.06800","publisher":"arXiv","source":"arXiv.org","title":"Diverse Demonstrations Improve In-context Compositional Generalization","type":"article","URL":"http://arxiv.org/abs/2212.06800"},
  {"id":"liuVisualInstructionTuning2023","abstract":"Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal ﬁeld. In this paper, we present the ﬁrst attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for generalpurpose visual and language understanding. Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instructionfollowing dataset. When ﬁne-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Liu","given":"Haotian"},{"family":"Li","given":"Chunyuan"},{"family":"Wu","given":"Qingyang"},{"family":"Lee","given":"Yong Jae"}],"citation-key":"liuVisualInstructionTuning2023","issued":{"date-parts":[["2023",4,17]]},"language":"en","number":"arXiv:2304.08485","publisher":"arXiv","source":"arXiv.org","title":"Visual Instruction Tuning","type":"article","URL":"http://arxiv.org/abs/2304.08485"},
  {"id":"luoUnderstandingDiffusionModels2022","abstract":"Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.","accessed":{"date-parts":[["2023",12,9]]},"author":[{"family":"Luo","given":"Calvin"}],"citation-key":"luoUnderstandingDiffusionModels2022","issued":{"date-parts":[["2022",8,25]]},"language":"en","number":"arXiv:2208.11970","publisher":"arXiv","source":"arXiv.org","title":"Understanding Diffusion Models: A Unified Perspective","title-short":"Understanding Diffusion Models","type":"article","URL":"http://arxiv.org/abs/2208.11970"},
  {"id":"malladiFineTuningLanguageModels2023","abstract":"Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Malladi","given":"Sadhika"},{"family":"Gao","given":"Tianyu"},{"family":"Nichani","given":"Eshaan"},{"family":"Damian","given":"Alex"},{"family":"Lee","given":"Jason D."},{"family":"Chen","given":"Danqi"},{"family":"Arora","given":"Sanjeev"}],"citation-key":"malladiFineTuningLanguageModels2023","issued":{"date-parts":[["2023",5,26]]},"language":"en","number":"arXiv:2305.17333","publisher":"arXiv","source":"arXiv.org","title":"Fine-Tuning Language Models with Just Forward Passes","type":"article","URL":"http://arxiv.org/abs/2305.17333"},
  {"id":"muennighoffScalingDataConstrainedLanguage2023","abstract":"The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Muennighoff","given":"Niklas"},{"family":"Rush","given":"Alexander M."},{"family":"Barak","given":"Boaz"},{"family":"Scao","given":"Teven Le"},{"family":"Piktus","given":"Aleksandra"},{"family":"Tazi","given":"Nouamane"},{"family":"Pyysalo","given":"Sampo"},{"family":"Wolf","given":"Thomas"},{"family":"Raffel","given":"Colin"}],"citation-key":"muennighoffScalingDataConstrainedLanguage2023","issued":{"date-parts":[["2023",6,15]]},"language":"en","number":"arXiv:2305.16264","publisher":"arXiv","source":"arXiv.org","title":"Scaling Data-Constrained Language Models","type":"article","URL":"http://arxiv.org/abs/2305.16264"},
  {"id":"nichaniHowTransformersLearn2024","abstract":"The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head [27]. We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.","accessed":{"date-parts":[["2024",9,18]]},"author":[{"family":"Nichani","given":"Eshaan"},{"family":"Damian","given":"Alex"},{"family":"Lee","given":"Jason D."}],"citation-key":"nichaniHowTransformersLearn2024","issued":{"date-parts":[["2024",8,13]]},"language":"en-US","number":"arXiv:2402.14735","publisher":"arXiv","source":"arXiv.org","title":"How transformers learn causal structure with gradient descent","type":"article","URL":"http://arxiv.org/abs/2402.14735"},
  {"id":"nicholImprovedDenoisingDiffusion2021","abstract":"Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modiﬁcations, DDPMs can also achieve competitive loglikelihoods while maintaining high sample quality. Additionally, we ﬁnd that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/ openai/improved-diffusion.","accessed":{"date-parts":[["2023",12,9]]},"author":[{"family":"Nichol","given":"Alex"},{"family":"Dhariwal","given":"Prafulla"}],"citation-key":"nicholImprovedDenoisingDiffusion2021","issued":{"date-parts":[["2021",2,18]]},"language":"en","number":"arXiv:2102.09672","publisher":"arXiv","source":"arXiv.org","title":"Improved Denoising Diffusion Probabilistic Models","type":"article","URL":"http://arxiv.org/abs/2102.09672"},
  {"id":"rafailovDirectPreferenceOptimization2023","abstract":"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF’s ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Rafailov","given":"Rafael"},{"family":"Sharma","given":"Archit"},{"family":"Mitchell","given":"Eric"},{"family":"Ermon","given":"Stefano"},{"family":"Manning","given":"Christopher D."},{"family":"Finn","given":"Chelsea"}],"citation-key":"rafailovDirectPreferenceOptimization2023","issued":{"date-parts":[["2023",5,29]]},"language":"en","number":"arXiv:2305.18290","publisher":"arXiv","source":"arXiv.org","title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","title-short":"Direct Preference Optimization","type":"article","URL":"http://arxiv.org/abs/2305.18290"},
  {"id":"schaefferAreEmergentAbilities2023","abstract":"Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing ﬁxed model outputs, emergent abilities appear due the researcher’s choice of metric rather than due to fundamental changes in model behavior with scale. Speciﬁcally, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous, predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and conﬁrm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and conﬁrm two predictions about metric choices in a metaanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Schaeffer","given":"Rylan"},{"family":"Miranda","given":"Brando"},{"family":"Koyejo","given":"Sanmi"}],"citation-key":"schaefferAreEmergentAbilities2023","issued":{"date-parts":[["2023",5,22]]},"language":"en","number":"arXiv:2304.15004","publisher":"arXiv","source":"arXiv.org","title":"Are Emergent Abilities of Large Language Models a Mirage?","type":"article","URL":"http://arxiv.org/abs/2304.15004"},
  {"id":"schickToolformerLanguageModels2023","abstract":"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacriﬁcing its core language modeling abilities.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Schick","given":"Timo"},{"family":"Dwivedi-Yu","given":"Jane"},{"family":"Dessì","given":"Roberto"},{"family":"Raileanu","given":"Roberta"},{"family":"Lomeli","given":"Maria"},{"family":"Zettlemoyer","given":"Luke"},{"family":"Cancedda","given":"Nicola"},{"family":"Scialom","given":"Thomas"}],"citation-key":"schickToolformerLanguageModels2023","issued":{"date-parts":[["2023",2,9]]},"language":"en","number":"arXiv:2302.04761","publisher":"arXiv","source":"arXiv.org","title":"Toolformer: Language Models Can Teach Themselves to Use Tools","title-short":"Toolformer","type":"article","URL":"http://arxiv.org/abs/2302.04761"},
  {"id":"shahLearningMixturesGaussians2023","abstract":"Recent works have shown that diﬀusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.","accessed":{"date-parts":[["2024",10,3]]},"author":[{"family":"Shah","given":"Kulin"},{"family":"Chen","given":"Sitan"},{"family":"Klivans","given":"Adam"}],"call-number":"arXiv","citation-key":"shahLearningMixturesGaussians2023","issued":{"date-parts":[["2023",7,3]]},"language":"en","number":"arXiv:2307.01178","publisher":"arXiv","source":"arXiv.org","title":"Learning mixtures of gaussians using the DDPM objective","type":"article","URL":"http://arxiv.org/abs/2307.01178"},
  {"id":"songScoreBasedGenerativeModeling2021","abstract":"Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient ﬁeld (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efﬁciency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high ﬁdelity generation of 1024 ˆ 1024 images for the ﬁrst time from a score-based generative model.","accessed":{"date-parts":[["2023",12,9]]},"author":[{"family":"Song","given":"Yang"},{"family":"Sohl-Dickstein","given":"Jascha"},{"family":"Kingma","given":"Diederik P."},{"family":"Kumar","given":"Abhishek"},{"family":"Ermon","given":"Stefano"},{"family":"Poole","given":"Ben"}],"citation-key":"songScoreBasedGenerativeModeling2021","issued":{"date-parts":[["2021",2,10]]},"language":"en","number":"arXiv:2011.13456","publisher":"arXiv","source":"arXiv.org","title":"Score-Based Generative Modeling through Stochastic Differential Equations","type":"article","URL":"http://arxiv.org/abs/2011.13456"},
  {"id":"staticChartero","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticChartero","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharteroa","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharteroa","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharterob","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharterob","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharteroc","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharteroc","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharterod","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharterod","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharteroe","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharteroe","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharterof","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharterof","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharterog","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharterog","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharteroh","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharteroh","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharteroi","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharteroi","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharteroj","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharteroj","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharterok","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharterok","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticCharterol","abstract":"Chartero记录的浏览历史数据。请勿删除本条目！（可以移动、改名）","author":[{"family":"static","given":"volatile"}],"citation-key":"staticCharterol","contributor":[{"literal":"Chartero"}],"genre":"JSON","title":"Chartero","type":"software","URL":"https://github.com/volatile-static"},
  {"id":"staticYueDuLiShiJiLu","abstract":"Chartero记录的阅读历史数据。\n请勿修改本条目！（可以移动）","archive_location":"http://zotero.org/users/12179261","author":[{"family":"static","given":"volatile"}],"citation-key":"staticYueDuLiShiJiLu","genre":"JSON","source":"我的文库","title":"阅读历史记录","title-short":"chartero","type":"software","URL":"https://github.com/volatile-static/Chartero"},
  {"id":"weiJailbrokenHowDoes2023","abstract":"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of “jailbreak” attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model’s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI’s GPT-4 and Anthropic’s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models’ red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity—that safety mechanisms should be as sophisticated as the underlying model—and argues against the idea that scaling alone can resolve these safety failure modes.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Wei","given":"Alexander"},{"family":"Haghtalab","given":"Nika"},{"family":"Steinhardt","given":"Jacob"}],"citation-key":"weiJailbrokenHowDoes2023","issued":{"date-parts":[["2023",7,5]]},"language":"en","number":"arXiv:2307.02483","publisher":"arXiv","source":"arXiv.org","title":"Jailbroken: How Does LLM Safety Training Fail?","title-short":"Jailbroken","type":"article","URL":"http://arxiv.org/abs/2307.02483"},
  {"id":"weiLargerLanguageModels2023","abstract":"We study how in-context learning (ICL) in language models is affected by semantic priors versus input–label mappings. We investigate two setups—ICL with ﬂipped labels and ICL with semantically-unrelated labels—across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with ﬂipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore ﬂipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input–label mappings shown in incontext exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classiﬁcation in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and ﬁnd that instruction tuning strengthens both the use of semantic priors and the capacity to learn input–label mappings, but more of the former.","accessed":{"date-parts":[["2024",3,14]]},"author":[{"family":"Wei","given":"Jerry"},{"family":"Wei","given":"Jason"},{"family":"Tay","given":"Yi"},{"family":"Tran","given":"Dustin"},{"family":"Webson","given":"Albert"},{"family":"Lu","given":"Yifeng"},{"family":"Chen","given":"Xinyun"},{"family":"Liu","given":"Hanxiao"},{"family":"Huang","given":"Da"},{"family":"Zhou","given":"Denny"},{"family":"Ma","given":"Tengyu"}],"citation-key":"weiLargerLanguageModels2023","issued":{"date-parts":[["2023",3,8]]},"language":"en","number":"arXiv:2303.03846","publisher":"arXiv","source":"arXiv.org","title":"Larger language models do in-context learning differently","type":"article","URL":"http://arxiv.org/abs/2303.03846"}
]
